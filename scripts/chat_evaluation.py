import pandas as pd
import os
import datetime
from dotenv import load_dotenv
from chatbot import Chatbot
from ragas.llms import LangchainLLMWrapper
from langchain_openai import ChatOpenAI
from ragas import evaluate
from datasets import Dataset  # Import Dataset from Hugging Face
from ragas.metrics import answer_relevancy, faithfulness, context_recall, context_precision, answer_correctness, answer_similarity

# Get api key
load_dotenv(".env")  # Load environment variables from the .env file
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')  # Fetch the OpenAI API key from environment

faiss_path = "faiss_index"  # Path to the FAISS index

# Initialize the Chatbot object with API key and FAISS index path
chatbot = Chatbot(  
        openai_api_key=OPENAI_API_KEY,   
        faiss_index_path=faiss_path
    )

# Function to apply RAGAS on the generated responses
def apply_ragas(df):
    """
    Applies the RAGAS (Retrieval-Augmented Generation for Answering Questions) evaluation on a given DataFrame of questions.
    This function processes each question in the DataFrame by simulating a chatbot interaction, capturing the chatbot's response,
    and creating a sample containing the question, context, ground truth, and generated answer. It also handles any errors that 
    occur during processing and logs the indices of questions that caused errors.
    
    Args:
        df (pd.DataFrame): A DataFrame containing the questions to be evaluated. It should have the following columns:
            - 'Pergunta': The question asked by the user.
            - 'Resposta Esperada': The expected correct answer.
    
    Returns:
        pd.DataFrame: A DataFrame containing the evaluation dataset with the following columns:
            - 'question': The question asked by the user.
            - 'contexts': A list of documents that contain the context for the answer.
            - 'ground_truth': The expected correct answer.
            - 'answer': The answer generated by the chatbot.
    """
    
    samples = []  # List to store evaluation samples
    error_questions = []  # List to track any questions that cause errors
    print("Number of questions: ", len(df))
    
    for idx, row in df.iterrows():  # Iterate through the DataFrame rows
        try:
            print(f"Processing question {idx+1}/{len(df)}")  # Print the current question index for tracking
            
            response, docs, chat_history = chatbot.run_chat_test(row['Pergunta'])  # Get the chatbot response and context
            
            # Create a sample with the question, context, ground truth answer, and generated answer
            sample = {
                'question': row['Pergunta'],  # User's question
                'contexts': [doc.page_content for doc in docs],  # Extract context documents
                'ground_truth': row['Resposta Esperada'],  # Expected correct answer
                'answer': response  # Chatbot's generated answer
            }
            
            samples.append(sample)  # Append the sample to the list
        
        except Exception as e:  # Handle errors in question processing
            error_questions.append(idx+1)  # Track the question index that caused the error
            print(f"Error processing question {idx+1}/{len(df)}: {row['Pergunta']}")
            print(f"Error: {e}")
            continue  # Skip to the next iteration if there's an error
    
    print('Error questions:', error_questions)  # Log the list of error questions
    
    # Create a DataFrame from the samples
    eval_dataset = pd.DataFrame(samples)
        
    return eval_dataset  # Return the evaluation dataset


def main(dataframe_path, test_id):
    """
    Main function to evaluate a ChatBot using RAGAS.
    
    Args:
        dataframe_path (str): Path to the CSV file containing the test data.
        test_id (str): Identifier for the test run.
    
    Returns:
        None
    
    This function performs the following steps:
    1. Reads the test data from the specified CSV file.
    2. Applies RAGAS to the generated responses.
    3. Converts the pandas DataFrame to a Hugging Face Dataset.
    4. Initializes the evaluator with a specified LLM model.
    5. Defines the metrics for evaluation.
    6. Evaluates the dataset using the specified metrics and LLM.
    7. Converts the evaluation results back to a pandas DataFrame.
    8. Prints a completion message.
    9. Saves the evaluation results to a CSV file.
    """
        
    print("\n----------------------------------------")
    print("Testing the ChatBot using RAGAS")
    print("----------------------------------------\n")
    
    # Read the test data from the CSV file
    qa_test = pd.read_csv(dataframe_path, encoding='ISO-8859-1')
    qa_test = qa_test[:2]  # Take the first two rows for testing
    
    # Apply RAGAS on the generated responses
    resultados_ragas = apply_ragas(qa_test)  
    
    # Convert the pandas DataFrame to Hugging Face Dataset format
    eval_dataset = Dataset.from_pandas(resultados_ragas)  
    
    # Initialize the LLM evaluator
    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model="gpt-4o")) 
    
    # Define the metrics to be used for evaluation
    metrics = [
        faithfulness,
        answer_relevancy,
        context_recall,
        answer_correctness,
        answer_similarity,
        context_precision
    ] 
    
    # Perform the evaluation using RAGAS
    results = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator_llm)  
    
    # Convert the results back to a pandas DataFrame
    df = results.to_pandas()  
    
    print("\nRAGAS for test dataset finished\n")  
    
    # Save the evaluation results to a CSV file
    df.to_csv(f"C:/Users/Melissa Freitas/Documents/Melissa/BCGx-Challenge2024/data/teste/results_ragas/results_ragas_{test_id}.csv", index=False)  

if __name__ == "__main__":
    # Path to your test dataset of questions and expected answers
    dataset_path = "C:/Users/Melissa Freitas/Documents/Melissa/BCGx-Challenge2024/data/teste/qa_for_test.csv"
    test_id = "context_size_1"  # Identifier for the test run
    main(dataset_path, test_id)  # Call the main function to start the evaluation
