import pandas as pd
import os
import datetime
from dotenv import load_dotenv
from chatbot import Chatbot
from ragas.llms import LangchainLLMWrapper
from langchain_openai import ChatOpenAI
from ragas import evaluate
from datasets import Dataset  # Importar o Dataset da Hugging Face
from ragas.metrics import answer_relevancy,faithfulness,context_recall,context_precision,answer_correctness,answer_similarity

# Get api key
load_dotenv(".env")
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')    

faiss_path = "faiss_index"

chatbot = Chatbot(  
        openai_api_key=OPENAI_API_KEY,   
        faiss_index_path = faiss_path
    )

# Function to apply RAGAS on the generated responses
def apply_ragas(df):
    """
    Applies the RAGAS (Retrieval-Augmented Generation for Answering Questions) evaluation on a given DataFrame of questions.
    This function processes each question in the DataFrame by simulating a chatbot interaction, capturing the chatbot's response,
    and creating a sample containing the question, context, ground truth, and generated answer. It also handles any errors that 
    occur during processing and logs the indices of questions that caused errors.
    Args:
        df (pd.DataFrame): A DataFrame containing the questions to be evaluated. It should have the following columns:
            - 'Pergunta': The question asked by the user.
            - 'Resposta Esperada': The expected correct answer.
    Returns:
        pd.DataFrame: A DataFrame containing the evaluation dataset with the following columns:
            - 'question': The question asked by the user.
            - 'contexts': A list of documents that contain the context for the answer.
            - 'ground_truth': The expected correct answer.
            - 'answer': The answer generated by the chatbot.
    """
    
    samples = []
    error_questions=[]
    print("Number of questions: ", len(df))
    
    for idx, row in df.iterrows():
        try:
            print(f"Processing question {idx+1}/{len(df)}")
            
            response, docs, chat_history = chatbot.run_chat_test(row['Pergunta'])
            
            # Criar uma amostra com a pergunta, contexto, resposta correta e resposta gerada
            sample = {
                'question': row['Pergunta'],  # Pergunta feita pelo usuário
                'contexts': [doc.page_content for doc in docs],  # Documento que contém a resposta
                'ground_truth': row['Resposta Esperada'],  # Resposta correta
                'answer': response  # Resposta do chatbot
            }
            
            # Adicionar a amostra à lista de samples
            samples.append(sample)
        
        
        except Exception as e:
            error_questions.append(idx+1)
            print(f"Error processing question {idx+1}/{len(df)}: {row['Pergunta']}")
            print(f"Error: {e}")
            continue
    
    print('Error questions:', error_questions)
    
    # Criar o dataset de avaliação com todas as amostras
    eval_dataset = pd.DataFrame(samples)
        
    return eval_dataset


def main(dataframe_path, test_id):
    """
    Main function to evaluate a ChatBot using RAGAS.
    Args:
        dataframe_path (str): Path to the CSV file containing the test data.
        test_id (str): Identifier for the test run.
    Returns:
        None
    This function performs the following steps:
    1. Reads the test data from the specified CSV file.
    2. Applies RAGAS to the generated responses.
    3. Converts the pandas DataFrame to a Hugging Face Dataset.
    4. Initializes the evaluator with a specified LLM model.
    5. Defines the metrics for evaluation.
    6. Evaluates the dataset using the specified metrics and LLM.
    7. Converts the evaluation results back to a pandas DataFrame.
    8. Prints a completion message.
    9. Saves the evaluation results to a CSV file.
    """
        
    print("\n----------------------------------------")
    print("Testing the ChatBot using RAGAS")
    print("----------------------------------------\n")
    
    qa_test = pd.read_csv(dataframe_path, encoding='ISO-8859-1')
    qa_test = qa_test[:2]
    
    # Aplicar RAGAS nas respostas geradas
    resultados_ragas = apply_ragas(qa_test)  
    
    # Converter o pandas DataFrame para o Dataset da Hugging Face
    eval_dataset = Dataset.from_pandas(resultados_ragas)  
    
    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model="gpt-4o")) 
    
    metrics = [
        faithfulness,
        answer_relevancy,
        context_recall,
        answer_correctness,
        answer_similarity,
        context_precision
    ] 
    
    # Avaliar utilizando RAGAS
    results = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator_llm)  
    
    df = results.to_pandas()  # Converter os resultados de volta para pandas   
    
    print("\nRAGAS for test dataset finished\n")  
    
    df.to_csv(f"C:/Users/Melissa Freitas/Documents/Melissa/BCGx-Challenge2024/data/teste/results_ragas/results_ragas_{test_id}.csv", index=False)  # Salvar os resultados em um arquivo CSV      

if __name__ == "__main__":
    # Carregar seu dataset de perguntas, respostas, documentos e página
    dataset_path = "C:/Users/Melissa Freitas/Documents/Melissa/BCGx-Challenge2024/data/teste/qa_for_test.csv"
    test_id = "context_size_1"
    main(dataset_path, test_id)
    