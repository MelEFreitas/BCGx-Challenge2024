# -*- coding: utf-8 -*-
"""BCGXChallange_V6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10xmRo3j6N-lapNn6zhRBuE4NM9suRogc
"""

import os
import re
import fitz
import tiktoken
import pytesseract
from PIL import Image
from datetime import datetime
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document # Import Document class

# Get api key
load_dotenv(".env")
# OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
embeddings = OpenAIEmbeddings()

# Função para adicionar metadados ao texto
def add_metadata(text, file_name, page_number):
    metadata = {
                    "file_name": file_name,
                    "page_number": page_number,
                    "text": text
                }
    return {"text": text, "metadata": metadata}

# Função para remover páginas indesejadas com base em palavras-chave
def is_relevant_page(text, keywords_to_remove=None):
    if keywords_to_remove is None:
        keywords_to_remove = ["sumário", "índice", "glossário"]  # Palavras-chave que indicam páginas indesejadas
    # Verifica se alguma das palavras-chave está presente na página (ignorando maiúsculas/minúsculas)
    for keyword in keywords_to_remove:
        if re.search(rf"\b{keyword}\b", text, re.IGNORECASE):
            return False  # Página não é relevante
    return True

# Função para limpar o texto, removendo caracteres especiais e espaços extras
def clean_text(text):
    # Remove quebras de linha, tabulações e substitui múltiplos espaços por um único espaço
    text = text.replace('\n', ' ').replace('\t', ' ')
    text = re.sub(r'\s+', ' ', text)  # Remove múltiplos espaços
    text = re.sub(r'\x0c', '', text) # Remover caracteres de controle como \x0c
    text = re.sub(r'\b\w{2,}\-\w{2,}\b', '', text) # Remover sequências de letras sem sentido (e.g., "OvOZ-TTOZ")
    text = re.sub(r'[|{}\[\]<>]', '', text)           # Remove caracteres não desejados
    # Adicione aqui mais limpezas personalizadas se necessário

    return text.strip()  # Remove espaços no início e fim

# Função para carregar e processar documentos PDF
def load_and_process_documents(file_paths):
    documents = []
    for file_path in file_paths:
        loader = PyMuPDFLoader(file_path)  # Carregar o PDF
        pages = loader.load()  # Carregar todas as páginas do PDF

        # Remove numeração de linhas no início da linha (ex: '1. texto' ou '1 texto')
        # pages = re.sub(r'^\d+\.\s*', '', pages, flags=re.MULTILINE)
        # pages = re.sub(r'^\d+\s+', '', pages, flags=re.MULTILINE)

        for page_number, page in enumerate(pages):
            text = page.page_content
            if is_relevant_page(text):  # Se a página for relevante
                cleaned_text = clean_text(text)  # Limpeza do texto
                document_with_metadata = add_metadata(cleaned_text, file_path, page_number + 1)  # Adicionar metadados
                documents.append(document_with_metadata)  # Adicionar à lista de documentos
    return documents

def extract_images_from_pdf(file_paths):
    extracted_images = []

    for pdf_file in file_paths:
    # Open PDF
      pdf_document = fitz.open(pdf_file)

      for page_num in range(pdf_document.page_count):
          page = pdf_document.load_page(page_num)
          images = page.get_images(full=True)

          for img_index, img in enumerate(images):
              xref = img[0]
              base_image = pdf_document.extract_image(xref)
              image_bytes = base_image["image"]
              image_ext = base_image["ext"]
              image_filename = f"image_page_{page_num + 1}_{img_index}.{image_ext}"

              # Salva a imagem no disco
              with open(image_filename, "wb") as image_file:
                  image_file.write(image_bytes)

              # Armazena as informações da imagem
              extracted_images.append({
                  "page": page_num + 1,
                  "image_filename": image_filename
              })

    return extracted_images

def extract_text_from_images(image_data):
    image_texts = []

    for image_info in image_data:
        image_filename = image_info["image_filename"]

        # Abre a imagem e aplica OCR para extrair texto
        image = Image.open(image_filename)
        text_from_image = pytesseract.image_to_string(image)

        # if text_from_image.strip():
        #     image_texts.append({
        #         "page": image_info["page"],
        #         "image_filename": image_filename,
        #         "text": text_from_image
        #     })

        if text_from_image.strip():
          metadata = {
                        "file_name": image_info["page"],
                        "page_number": image_filename,
                        "text": text_from_image
                        }
          image_texts.append({
              "text": text_from_image,
              "metadata": metadata
          })

        # if text_from_image.strip():
            # image_texts.append(
            #     text_from_image
            # )

    return image_texts

def create_embeddings(document, embeddings) -> list:
    """
    Recebe um documento com texto e metadados, divide em chunks, gera embeddings
    e retorna uma lista de objetos Document contendo os chunks e seus embeddings.
    """
    text = document['text']  # Extrai o texto limpo do documento
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)
    documents = []  # Lista para armazenar os objetos Document
    text_split = text_splitter.split_text(text)

    # Inicializa o codificador de tokens
    encoding = tiktoken.encoding_for_model("gpt-4")

    for t in text_split:
        # Conta os tokens do chunk
        token_count = len(encoding.encode(t))
        print(f"Chunk length in tokens: {token_count}")

        # Gera o embedding para o chunk
        emb = embeddings.embed_query(t)

        # Armazena o embedding e metadados no formato correto
        documents.append(
            Document(
                page_content=t,
                metadata={
                    "embedding": emb,
                    "file_name": document["metadata"]["file_name"],
                    "page_number": document["metadata"]["page_number"]
                }
            )
        )

    return documents

# def create_image_embeddings(text:str)->list:
def create_image_embeddings(document, embeddings) -> list:

    """
    Splits the text into chunks, generates embeddings for each chunk,
    and returns a list of Document objects containing the chunks and their embeddings.
    """
    text = document['text']
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)
    documents = [] # List to store Document objects
    text_split = text_splitter.split_text(text)

    # Inicializa o codificador de tokens
    encoding = tiktoken.encoding_for_model("gpt-4")

    for t in text_split:
      # Conta os tokens do chunk
        token_count = len(encoding.encode(t))
        print(f"Chunk length in tokens: {token_count}")

        emb = embeddings.embed_query(t) # Generate embedding for the chunk
        # documents.append(Document(page_content=t, metadata={})) # Create Document objects

        # Armazena o embedding e metadados no formato correto
        documents.append(
            Document(
                page_content=t,
                metadata={
                    "embedding": emb,
                    "file_name": document["metadata"]["file_name"],
                    "page_number": document["metadata"]["page_number"]
                }
            )
        )

    return documents

# Função para salvar os documentos no FAISS
def save_to_faiss(documents, embeddings):
    # Cria uma lista de textos e metadados para o FAISS
    texts = [doc.page_content for doc in documents]

    # Atualiza a coleta de metadados para incluir página e nome do arquivo
    metadatas = [
        {
            "file_name": doc.metadata["file_name"],
            "page_number": doc.metadata["page_number"],
            "embedding": doc.metadata["embedding"]
        }
        for doc in documents
    ]

    # Cria o banco de dados FAISS com textos e metadados atualizados
    vector_store = FAISS.from_texts(texts, embeddings, metadatas=metadatas)

    # Salva o banco de dados FAISS
    vector_store.save_local("faiss_index")
    print(f"Saved {len(documents)} documents to FAISS.")

# Exemplo de uso
def main():
    data_dir = "C:/Users/Melissa Freitas/Documents/Melissa/BCGx-Challenge2024/data"
    # data_dir = '/content/docs'

    if not os.path.exists(data_dir):
        print(f"Directory {data_dir} does not exist.")
        return

    file_paths = [os.path.join(data_dir, pdf_file) for pdf_file in os.listdir(data_dir) if pdf_file.endswith(".pdf")]
    documents = load_and_process_documents(file_paths)

    # Inicializa os embeddings
    embeddings = OpenAIEmbeddings()
    # embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key)

    # Cria embeddings dos textos para os documentos processados
    all_documents = []
    for doc in documents:
        print(f"Processing {doc['metadata']['file_name']}...")

        # Agora passa o dicionário inteiro (doc), não apenas o texto
        chunk_text_embeddings = create_embeddings(doc, embeddings)
        all_documents.extend(chunk_text_embeddings)

    print("Embeddings de texto gerados com sucesso")

    #
    cleaned_image_documents = []
    image_data = extract_images_from_pdf(file_paths)
    image_texts = extract_text_from_images(image_data)

    # Limpa o texto extraído das imagens
    cleaned_image_documents = [{'text': clean_text(item['text']), 'metadata': item['metadata']} for item in image_texts]

    # Cria embeddings das imagens para os documentos processados
    for i, text in enumerate(cleaned_image_documents):
        print(f"Processing {text['metadata']['file_name']}...")
        chunk_image_embeddings = create_image_embeddings(text, embeddings)
        all_documents.extend(chunk_image_embeddings)

    print("Embeddings de imagem gerados com sucesso")

    # Salva os documentos no FAISS
    print("Salvando FAISS")
    save_to_faiss(all_documents, embeddings)

if __name__ == "__main__":
    main()